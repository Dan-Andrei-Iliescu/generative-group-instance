{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "from model import experimental2d_model, grapher\n",
    "from data import loader\n",
    "from helpers import helpers, metrics\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_position(t, d, TΔmin, Tmax):  # return.shape=(T,B,d)\n",
    "    # t.shape=(T,B)   T=sequence_length, B=batch_size\n",
    "    \"\"\"A position-embedder, similar to the Attention paper, but tweaked to account for\n",
    "    floating point positions, rather than integer.\n",
    "    \"\"\"\n",
    "    R = Tmax / TΔmin * 100\n",
    "    drange_even = TΔmin * R**(np.arange(0,d,2)/d)\n",
    "    drange_odd = TΔmin * R**((np.arange(1,d,2) - 1)/d)\n",
    "    x = np.concatenate([np.sin(t[:,:,None] / drange_even), np.cos(t[:,:,None] / drange_odd)], 2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('/home/omernivr/Downloads/riverflow/maurer.pickle.npy', allow_pickle= True)\n",
    "df_attributes = np.load('/home/omernivr/Downloads/riverflow/attributes.pickle', allow_pickle= True)\n",
    "att_csv = pd.read_csv('/home/omernivr/Downloads/riverflow/data/attibutes.csv')\n",
    "basin_list = pd.read_csv('/home/omernivr/Downloads/riverflow/data/basin_list.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['p_mean', 'pet_mean', 'p_seasonality', 'frac_snow',\n",
    "               'aridity', 'high_prec_freq', 'high_prec_dur', \n",
    "               'low_prec_freq', 'low_prec_dur', \n",
    "               'carbonate_rocks_frac', \n",
    "                'geol_permeability', \n",
    "               'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity','soil_conductivity', \n",
    "                'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac',  \n",
    "                'elev_mean', 'slope_mean', \n",
    "                'area_geospa_fabric', \n",
    "                'frac_forest', 'lai_max', 'lai_diff', 'gvf_max','gvf_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_list = basin_list[0].apply(lambda x: '0' + str(x) if len(str(x)) < 8 else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_csv1 = att_csv[attributes]\n",
    "att_csv1 = (att_csv1 - np.mean(att_csv1, axis  = 0)) / np.std(att_csv1, axis =0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ymd = df['01013500'].groupby(['Year', 'Mnth']).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymd['d_cumsum'] = 0\n",
    "for y in ymd['Year']:\n",
    "    temp = ymd.loc[ymd['Year'] == y, 0].cumsum()\n",
    "    ymd.loc[ymd['Year'] == y,'d_cumsum'] =  np.concatenate(([0], temp[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in df.items():\n",
    "    df[k].reset_index(inplace=True)\n",
    "    df[k] = pd.DataFrame.merge(v, ymd, right_on=['Year', 'Mnth'], left_on=['Year', 'Mnth'])\n",
    "    df[k]['n_day'] = df[k]['d_cumsum_x'] + df[k]['Day']\n",
    "    df[k]['t'] = df[k]['Year'] + (df[k]['n_day'] - 1) / 366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = {}\n",
    "for (b) in basin_list: \n",
    "    if (df[b].shape[0] < 10593):\n",
    "        basin_list = basin_list[basin_list!=b]\n",
    "        continue\n",
    "    df_filtered[b] = df[b][['Date','t', 'Dayl(s)', 'PRCP(mm/day)', 'SRAD(W/m2)', 'Tmax(C)', 'Tmin(C)', 'Vp(Pa)', 'Q']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Choose type of transform, i.e., 'standardize' or 'normalize' \n",
    "dist = 'gaussian'\n",
    "\n",
    "if dist == 'gaussian':\n",
    "    transform = 'standardize'\n",
    "    log_P = True\n",
    "    log_Q = True\n",
    "    \n",
    "if dist == 'gamma':\n",
    "    transform = 'normalize'\n",
    "    log_P = True\n",
    "    log_Q = True\n",
    "    gamma_shift = 1e-3\n",
    "\n",
    "divide_by_area = True\n",
    "cols = ['t','Dayl(s)', 'PRCP(mm/day)', 'SRAD(W/m2)', 'Tmax(C)', 'Tmin(C)', 'Vp(Pa)', 'Q']\n",
    "epsilon = 1e-3\n",
    "\n",
    "x_maxs, x_mins, x_means, x_stds = [], [], [], []\n",
    "for k,v in df_filtered.items():\n",
    "#     # Scale streamflow values by catchment area\n",
    "    if divide_by_area:\n",
    "        v['Q'] = v['Q']/df_attributes[k]['area_geospa_fabric'].values\n",
    "    \n",
    "    # Calculate mean (after scaling by area)\n",
    "#     v['Q_mu'] = v['Q'].mean()\n",
    "    \n",
    "#     Log-transform precipitation\n",
    "    if log_P: \n",
    "        v['PRCP(mm/day)'] = np.log(v['PRCP(mm/day)'] + epsilon)\n",
    "    \n",
    "    # Log-transform streamflow\n",
    "    if log_Q: \n",
    "        v['Q'] = np.log(v['Q'] + epsilon)\n",
    "    \n",
    "#     x_maxs.append(v[cols].max().values)\n",
    "#     x_mins.append(v[cols].min().values)\n",
    "    x_means.append(v[cols].values)\n",
    "    x_stds.append(v[cols].values)\n",
    "\n",
    "# x_max = np.concatenate(x_maxs).reshape(-1,len(cols)).max(axis=0)\n",
    "# x_min = np.concatenate(x_mins).reshape(-1,len(cols)).min(axis=0)\n",
    "x_mean = np.concatenate(x_means, axis=0).mean(axis=0)\n",
    "x_std = np.concatenate(x_stds, axis=0).std(axis = 0)\n",
    "\n",
    "for k,v in df_filtered.items():\n",
    "    for i, col in enumerate(cols):\n",
    "        if transform == 'normalize':\n",
    "            \n",
    "            v[col] = (v[col] - x_min[i]) / (x_max[i] - x_min[i])\n",
    "            \n",
    "            if dist==\"gamma\":\n",
    "                v['Q'] = v['Q'] + gamma_shift\n",
    "            \n",
    "            def rev_transform(x):\n",
    "                x = x * (x_max[0] - x_min[0]) + x_min[0]\n",
    "                if log_Q:\n",
    "                    x = np.exp(x) - epsilon\n",
    "                if dist == \"gamma\":\n",
    "                    x = x - gamma_shift\n",
    "                return x\n",
    "            \n",
    "            def rev_transform_tensor(x):\n",
    "                x = x * (x_max[0] - x_min[0]) + x_min[0]\n",
    "                if log_Q:\n",
    "                    x = torch.exp(x) - epsilon \n",
    "                if dist == \"gamma\":\n",
    "                    x = x - gamma_shift\n",
    "                return x\n",
    "        \n",
    "        elif transform == 'standardize':\n",
    "            \n",
    "            v[col] = (v[col] - x_mean[i]) / x_std[i]\n",
    "            \n",
    "            #WARNING -- NO GAMMA SHIFT\n",
    "            \n",
    "            def rev_transform(x):\n",
    "                x = x * x_std[0] + x_mean[0]\n",
    "                if log_Q:\n",
    "                    x = np.exp(x) - epsilon\n",
    "                if dist == \"gamma\":\n",
    "                    x = x - gamma_shift\n",
    "                return x\n",
    "\n",
    "            def rev_transform_tensor(x):\n",
    "                x = x * x_std[0] + x_mean[0]\n",
    "                if log_Q:\n",
    "                    x = torch.exp(x) - epsilon \n",
    "                if dist == \"gamma\":\n",
    "                    x = x - gamma_shift\n",
    "                return x\n",
    "        \n",
    "        else:\n",
    "            print(\"No transform has been applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt_sliced = {}\n",
    "for b in basin_list:\n",
    "    df_filt_sliced[b] = {}\n",
    "    temp = df_filtered[b].reset_index()[:10500]\n",
    "    \n",
    "    # Use \"499\" not \"500\" -- since it is a pandas dataframe indexing\n",
    "    t_y = np.array([embed_position((temp.loc[j:j+499, 't'])[:, None], d=122, TΔmin= 0.2, Tmax=180) for j in range(0, 10499, 500)])\n",
    "    df_filt_sliced[b]['x'] = temp[['PRCP(mm/day)','Dayl(s)', 'SRAD(W/m2)', 'Tmax(C)', 'Tmin(C)', 'Vp(Pa)']]\n",
    "#     df_filt_sliced[b]['x']['basin'] = b\n",
    "    df_filt_sliced[b]['t'] = t_y\n",
    "    df_filt_sliced[b]['y'] = temp['Q']\n",
    "    df_filt_sliced[b]['date'] = temp['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step, test_step, train_loss, test_loss, m_tr, m_te = grapher.build_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_list = basin_list.reset_index()\n",
    "all_idx = np.arange(len(basin_list))\n",
    "train_b_idx = np.random.choice(all_idx, 400, replace = False)\n",
    "test_b_idx = all_idx[~np.isin(all_idx, train_b_idx).reshape(-1)]\n",
    "valid_b_idx = test_b_idx[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = False\n",
    "save_dir = os.path.expanduser('~/Downloads/riverflow/river_flow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    step = 0\n",
    "    # change to run 9 if you want to overfit\n",
    "    EPOCHS = 75; batch_s  = 16; run = 52; tr_regime ='shuffle'\n",
    "    l= [128, 64, 32]; heads = 16; e = 128; context = 400; c= 400\n",
    "    name_comp = 'run_' + str(run)\n",
    "    logdir = save_dir + '/logs/' + name_comp\n",
    "    writer = tf.summary.create_file_writer(logdir)\n",
    "    folder = save_dir + '/ckpt/check_' + name_comp\n",
    "    optimizer_c = tf.keras.optimizers.Adam(3e-4)\n",
    "    helpers.mkdir(folder)\n",
    "    decoder = experimental2d_model.Decoder(e, l[0], l[1], l[2], num_heads=heads)\n",
    "    num_batches = 500\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer_c, net=decoder)\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                \n",
    "                # choose the basins in each batch     \n",
    "                batch_idx = np.random.choice(train_b_idx, 16)\n",
    "                \n",
    "                # choose the block of time \n",
    "                block_idx = np.random.choice(np.arange(21), 1)\n",
    "                \n",
    "                                \n",
    "                y_tr = (np.array([df_filt_sliced[basin_list.iloc[batch_idx[i]][0]]['y'] for i in range(16)])).reshape(16, 21, 500)[:, block_idx, :]\n",
    "            \n",
    "                x_tr = np.array([np.concatenate(\n",
    "                                (np.array(df_filt_sliced[basin_list.iloc[batch_idx[i]][0]]['x']).reshape(21, 500, -1), \n",
    "                                                 df_filt_sliced[basin_list.iloc[batch_idx[i]][0]]['t'].squeeze()), axis = 2) \n",
    "                                 for i in range(16)])[:, block_idx, :, :]\n",
    "                \n",
    "                \n",
    "                to_gather = helpers.gather_idx(c, l=500, b=16)\n",
    "                \n",
    "                temp = np.zeros((16, 500))\n",
    "                \n",
    "                temp[to_gather[:, 0], to_gather[:, 1]] = 1\n",
    "                \n",
    "                \n",
    "                pred, pred_log, weights, names, shapes, y_real, g = train_step(decoder, optimizer_c, train_loss, m_tr, x_tr.squeeze(), y_tr.squeeze(), d = True, to_gather=temp)\n",
    "                if (epoch == 0) & (batch_n == 0): helpers.write_speci(folder, names, shapes, context, heads)\n",
    "                if batch_n % 3 == 0:\n",
    "                    m_te.reset_states(); test_loss.reset_states()\n",
    "\n",
    "\n",
    "                    y_te = np.array([df_filt_sliced[basin_list.iloc[valid_b_idx[i]][0]]['y'] for i in range(len(valid_b_idx))]).reshape(-1, 21, 500)[:, block_idx, :]\n",
    "                    \n",
    "                    \n",
    "                    x_te = np.array([np.concatenate(\n",
    "                                (np.array(df_filt_sliced[basin_list.iloc[valid_b_idx[i]][0]]['x']).reshape(21, 500, -1), \n",
    "                                                 df_filt_sliced[basin_list.iloc[valid_b_idx[i]][0]]['t'].squeeze()), axis = 2) \n",
    "                                 for i in range(len(valid_b_idx))])[:, block_idx, :, :]\n",
    "\n",
    "                    to_gather_te = helpers.gather_idx(c, l=500, b= len(valid_b_idx))\n",
    "                    temp_te = np.zeros((len(valid_b_idx), 500))\n",
    "                    temp_te[to_gather_te[:, 0], to_gather_te[:, 1]] = 1\n",
    "                    pred_te, pred_log_te = test_step(decoder, test_loss, m_te, x_te =x_te.squeeze(), y_te = y_te.squeeze(), to_gather=temp_te, d=True)\n",
    "\n",
    "                    fig,ax = plt.subplots(figsize=(12, 8))\n",
    "                    idx_p = np.random.choice(np.arange(len(valid_b_idx)), 1)\n",
    "                    \n",
    "                    ax.scatter(df_filt_sliced[basin_list.iloc[int(valid_b_idx[idx_p])][0]]['date'][500 * int(block_idx) : 500 * (int(block_idx)+1)], y_te[1], c='blue')\n",
    "                    ax.scatter(df_filt_sliced[basin_list.iloc[int(valid_b_idx[idx_p])][0]]['date'][500 * int(block_idx) : (500 * int(block_idx) + 400)], y_te[1][:, :400], c='red')\n",
    "                    ax.scatter(df_filt_sliced[basin_list.iloc[int(valid_b_idx[idx_p])][0]]['date'][500 * int(block_idx) + 400: 500 * (int(block_idx)+1)], pred_te[1, 399:], c='goldenrod') \n",
    "                    plt.show()\n",
    "                        \n",
    "                        \n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
